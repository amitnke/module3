search()
ls()
cube
square
help ls
make.NegLogLikk <- function(data, fixed = c(FALSE, FALSE)){
params <- fixed
function(p){
params[!fixed] <- p
mu <- params[1]
sigma <- params[2]
## Calculate the Normal density
a <- -0.5*length(data)*log(2*pi*sigma^2)
b <- -0.5*sum((data-mu)^2) / (sigma^2)
-(a + b)
}
}
set.seed(1)
mormals <- rnorm(100, 1, 2)
nLL <- make.NegLogLikk(mormals)
nLL
ls(environment(nLL))
optim(c(mu = 0, sigma = 1), nLL)$par
seed(1)
get.seed()
help set.seed()
help set.sead
x <- as.Date("1970-01-01")
x
unclass(x)
x
unclass(as.Date("1970-01-02"))
Sys.time()
class(x)
x <- Sys.time()
x
class(x)
p <- as.POSIXct(x)
p
names(unclass(p))
p <- as.POSIXlt(x)
p
names(unclass(p))
p$wday
p$sec
p$hour
p$min
p$mon
p$yday
p$isdst
p
x
class(x)
unclass(x)
datestring <- c("January 10, 2012 10:40", December 9, 2011 9:10)
datestring <- c("January 10, 2012 10:40", "December 9, 2011 9:10")
datastring
datestring
x <- strptime(datestring, "%B %d, %Y %H:%M")
x
class(x)
p <- as.POSIXlt(x)
p
unclass(p)
x <- Sys.time()
x
class(x)
y <- as.Date("2015-05-13 02:23:32")
y
unclass(x)
unclass(x)
p <- as.POSIXlt(x)
names(unclass(p))
p$sec
x <- Sys.Date()
x
datestring()
datestring
search()
Sys.getenvironment()
ls(environment)
ls(environment(GlobalEnv))
ls(environment("GlobalEnv"))
ls(environment(".GlobalEnv"))
names()
names
function(x)
{}
x
install.packages("dplyr")
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
"Hello" + "World"
?paste
paste(2,3)
paste(2, 3, sep = "and")
paste(2, 3, sep = " and ")
?collapse
??collapse
arr <- c(10,20,30,40)
class(arr)
arr
arr + c(1,2)
b > 20
b <- arr > 20
arr[b]
---Preprocessing for kNN algorithm
data(iris)
str(iris)
---Which species of flower is based on remaining feature
table(iris$Species)
head(iris)
----Mix up the rows as data seems to be ordered with Species
set.seed(9850)
gp <- runif(150)
iris <- iris[order(gp),]
str(iris)
head(iris,10)
---rescale my numerical vector
summary(iris[,c(1,2,3,4)])
---all the column has different range. All feature to be scaled in similar fashion.
--Normalize, value - min / (max - min)
normalize <- function(x) { return ((x-min(x))/(max(x) - min(x))) }
normalize(c(1,2,3,4))
--Now normalize the sepal length, width, petal length, width
summary(iris_n)
kmeans.wss.k <- function(D, k){
km <- kmeans(D, centers = k, nstart = 5)
return (km$tot.withinss)
}
kmeans.wss.k(tastes, 4)
kmeans.wss.k(tastes, 5)
kmeans.wss.k(tastes, 7)
kmeans.wss.k(tastes, 8)
kmeans.wss.k(tastes, 10)
D <- read.table("whiskies.txt",header = T, sep = ",")
#Data is from Scotland Whisky
class(D)
str(D)
#Distillary information along with Bunch of testing score in range of 0-4.
#Also location information of location of distillary
#Each record is one distiallry information
#what the whisky that have similar taste. Also we will try to find the correlation between
#location and whisky taste
tastes = D[,3:14]
str(tastes)
#Devide the datastet into 4 cluster and 5 starting point
kmfit = kmeans(tastes, centers = 4, nstart = 5)
class(kmfit)
str(kmfit)
# Cluster - Number of cluster to which dataset belongs
# centers - k Center(finding k cluster)
# totss - The sum of total of squre(Distance)
# withinss - Vector of within-cluster sum of squares, one component per cluster.
# tot.withinss(distortion) - Total within-cluster sum of squares, i.e. sum(withinss). The sum of all withinss
# betweenss - The between-cluster sum of squares, i.e. totss-tot.withinss.
# Size - The number of points in each cluster.
# iter - Number of iteration to converge the cluster
# ifault - integer: indicator of a possible algorithm problem â€“ for experts.
kmfit$centers
kmfit$size
#Plots every combination of two dimensions
plot(tastes)
plot(kmfit)
kmfit$cluster==3
D[kmfit$cluster==3,]
kmfit2 = kmeans(tastes, centers = 4, nstart = 10)
kmfit2$size
#Add another column to store cluster information
D$cluster = kmfit$cluster
str(D)
write.csv(D,"whiskies_post_analsysis.csv")
kmeans.wss.k <- function(D, k){
km <- kmeans(D, centers = k, nstart = 5)
return (km$tot.withinss)
}
D <- read.table("whiskies.txt",header = T, sep = ",")
#Data is from Scotland Whisky
class(D)
str(D)
#location and whisky taste
tastes = D[,3:14]
str(tastes)
#Devide the d
plot(x^2 + 2^x + 1)
plot(log)
plot(ln)
plot(exp)
plot(sin)
plot(cos)
plot(tan)
library(twitterR)
library("twitteR", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(tm)
install.packages("tm")
install.packages("tm")
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:NLP", unload=TRUE)
library("NLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(NLP)
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
api_key <- "x3JPfnDiFx61mURPDlKPKzPe8"
api_secret <- "xbo1ZDSGcHP4qYp24nK4slgFsxvml5TYLQyMOZy1ciTw5uQn5a"
access_token <- "115878452-h21LayhtJbXd4BS0y9G3316zdM5J09dDkpfvNQxq"
access_token_secret <- "fsvLh86IRFUEKhGUKZgDmyV3fJWtP4vfbSI9O8RlmcHri"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
rdmTweets = userTimeline("rdatamining", n=100)
rdmTweets
yTweets = userTimeline("Yahoo", n=500)
tTweets
yTweets
rdmTweets
tweets = fdaTweets
fdaTweets
fdaTweets = userTimeline("FDArecalls", n=500)
fdaTweets
tweets = fdaTweets
search()
str(tweets)
df <- do.call("rbind", lapply(tweets, as.data.frame))
str(df)
df$text
myCorpus = Corpus(VectorSource(df$text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myStopwords <- c(stopwords('english'), "available", "via", "obama", "modi", "sonia", "loksabha")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
inspect(myCorpus[1:3])
myDtm <- DocumentTermMatrix(myCorpus)
dim(myDtm)
myDtm <- DocumentTermMatrix(myCorpus)
dictCorpus <- myCorpus
df$text[1]
inspect(myCorpus[1])
myDtm <- DocumentTermMatrix(myCorpus)
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
dictCorpus <- myCorpus
myDtm <- DocumentTermMatrix(myCorpus)
data(iris)
str(iris)
head(iris,2)
table(iris$Species)
head(iris)
?set.seed
summary(iris)
str(iris)
table(iris$Species)
head(iris)
gp <- runif(150)
gp
order(gp)
gp[order(gp)]
iris[order(gp)]
order(gp)
iris[order(gp),]
iris <- iris[order(gp),]
iris
str(iris)
head(iris)
summary(iris[,c(1,2,3,4)])
normalize <- function(x) { return ((x-min(x))/(max(x) - min(x))) }
source('~/R/k-Nearest-Neighbour/kNN_iris.R', echo=TRUE)
normalize(c(1,2,3,4))
summary(iris_n)
iris_n <- sapply(normalize, iris[,c(1,2,3,4)])
iris_n <- sapply(normalize, iris[-5])
iris_n <- sapply(normalize, iris(-5))
iris_n <- sapply(normalize, iris[-5])
iris[-5]
iris_n <- normalize(iris[-5])
summary(iris_n)
iris_train <- iris_n[1:129,]
iris_test <- iris_n[130:150,]
iris_train_target <- iris[1:129, 5]
iris_test_target <- iris[130:150,5]
sqrt(150)
m1 <- knn(train=iris_train, test = iris_test, cl=iris_train_target, k=13)
?knn
library(knn)
install.packages("knn")
library(MASS)
m1 <- knn(train=iris_train, test = iris_test, cl=iris_train_target, k=13)
library(ggvis)
library(class)
m1 <- knn(train=iris_train, test = iris_test, cl=iris_train_target, k=13)
m1
plot(m1)
library(gmodels)
CrossTable(x = iris_test_target, y = m1,
prop.chisq=FALSE)
library(dplyr)
# Make sure we have all the logs -----------------------------------------------
message("Downloading logs")
#start <- Sys.Date() - 31
start <- as.Date('2015-06-16')
yesterday <- Sys.Date() - 1
days <- seq(start, yesterday, by = 'day')
years <- as.POSIXlt(days)$year + 1900
urls  <- paste0('http://cran-logs.rstudio.com/', years, '/', days, '.csv.gz')
paths <- paste0("logs/", days, ".csv.gz")
if (!file.exists("logs")) dir.create("logs")
missing <- !(paths %in% dir("logs", full.name = TRUE))
ok <- Map(download.file, urls[missing], paths[missing])
library("RPostgreSQL", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library("hflights", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
hflights <- hflights_postgress() %>% tbl("hflights")
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv)
con <- dbConnect(drv)
con <- dbConnect(drv, dbname = "amitkumar")
con <- dbConnect(drv, dbname = "amitkumar")
con <- dbConnect(drv, dbname = "R_Projects")
con <- dbConnect(drv, dbname = "R_Project")
con <- dbConnect(drv, dbname = "tempdb")
dbListConnection(drv, ...)
dbGetInfo(drv)
con <- dbConnect(drv, dbname="")
con <- dbConnect(drv, dbname="tempdb")
?dbConnect
hflights <- hflights_postgress() %>% tbl("hflights")
con <- dbConnect(drv, dbname = "tempdb")
con <- dbConnect(drv, dbname = "tempdb")
con <- dbConnect(drv, dbname="R_Project")
con <- dbConnect(drv, dbname="R")
con <- dbConnect(drv, dbname = "R_Project")
datasetsDb()
con <- dbConnect(drv, dbname = ":memory:")
con <- dbConnect(drv, dbname = "postgres")
con <- dbConnect(drv, dbname = "template1")
con <- dbConnect(drv, dbname = "template1")
con <- dbConnect(drv, dbname = "hodley")
search()
drv <- dbDriver("RPostgreSQL")
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "hodley")
head(flights)
install.packages("hflights")
install.packages("hflights")
library(hflights)
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "hodley")
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
setwd("~/R/Module3")
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
summary(wbcd_n[c("radius_mean", "area_mean", "smoothness_mean")])
summary(wbcd_n$area_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k=21)
data(iris)
table(iris$Species)
head(iris)
gp <- runif(150)
iris <- iris[order(gp),]
str(iris)
head(iris,10)
summary(iris[,c(1,2,3,4)])
normalize <- function(x) { return ((x-min(x))/(max(x) - min(x))) }
normalize(c(1,2,3,4))
iris[-5]
iris_n <- normalize(iris[-5])
summary(iris_n)
setwd("~/R/kMeans")
D <- read.table("whiskies.txt",header = T, sep = ",")
class(D)
str(D)
tastes = D[,3:14]
str(tastes)
str(D)
kmfit = kmeans(tastes, centers = 4, nstart = 5)
class(kmfit)
str(kmfit)
Iris <- iris
Iris.features = Iris
Iris
Iris.feature$Species <- NULL
Iris.features$Species <- NULL
?kmeans
?kmeans
library(devtools)
library(rjson)
library(bit64)
search()
library(httr)
library(twitteR)
library(devtools)
library(rjson)
library(bit64)
library(httr)
library(twitteR)
api_key <- "x3JPfnDiFx61mURPDlKPKzPe8"
api_secret <- "xbo1ZDSGcHP4qYp24nK4slgFsxvml5TYLQyMOZy1ciTw5uQn5a"
access_token <- "115878452-h21LayhtJbXd4BS0y9G3316zdM5J09dDkpfvNQxq"
access_token_secret <- "fsvLh86IRFUEKhGUKZgDmyV3fJWtP4vfbSI9O8RlmcHri"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
searchTwitter("iphone")
user <- getUser("agrawalsamit")
userFriends <- user$getFriends(n=5000) #put () if you want to get all friends and followers
userFollowers <- user$getFollowers(n=5000)
userNeighbors <- union(userFollowers, userFriends)
userNeighbors.df = twListToDF(userNeighbors)
library(RCurl)
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
userNeighbors.df[userNeighbors.df=="0"]<-1
userNeighbors.df$logFollowersCount <-log(userNeighbors.df$followersCount)
userNeighbors.df$logFriendsCount <-log(userNeighbors.df$friendsCount)
kObject.log <- data.frame(userNeighbors.df$logFriendsCount,userNeighbors.df$logFollowersCount)
mydata <- kObject.log
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
userMeans.log <- kmeans(kObject.log, centers=4, iter.max=10, nstart=100)
require(rCharts)
kObject.log$cluster=factor(userMeans.log$cluster)
userNeighbors.df$cluster <- kObject.log$cluster
p2 <- nPlot(logFollowersCount ~ logFriendsCount, group = 'cluster', data = userNeighbors.df, type = 'scatterChart')
setwd("~/R/Module3/edureka-text-clustering-dataset")
list.files()
tweets <- read.csv("Tweets.csv")
head(tweets)
tweets <- read.csv("Tweets.csv", header = T)
head(tweets)
?Corpus
search()
library(tm)
library(tm)
head(tweets)
?Corpus
tweets$content
VectorSource(tweets$content)
a <- VectorSource(tweets$content)
tweets1 <- Corpus(a)
tweets1
inspects(tweets)
inspect(tweets1)
head(inspect(tweets1))
inspect(tweets1)
tweets2 <- tm_map(tweets1, tolower)
tweets2
tweets2$content
tweets2[[1]]
tweets3 <- tm_map(tweets2, removeWords, stopwords("english"))
tweets3[[173]]
tweets4 <- tm_map(tweets3, removePunctuation)
tweets4[[173]]
tweets5 <- tm_map(tweets4, removeNumbers)
tweets[[173]]
tweets5[[173]]
tweets6 <- tm_map(tweets5, stripWhitespace)
tweets6[[173]]
dtm <- DocumentTermMatrix(tweets6)
dtm<-DocumentTermMatrix(tweets6)
dtm <- DocumentTermMatrix(tweets6)
tm_map(tweets6, PlainTextDocument)
dtm <- DocumentTermMatrix(tweets6)
tweets2 <- tm_map(tweets1, content_transformer(tolower))
tweets3 <- tm_map(tweets2, removeWords, stopwords("english"))
tweets4 <- tm_map(tweets3, removePunctuation)
tweets5 <- tm_map(tweets4, removeNumbers)
tweets6 <- tm_map(tweets5, stripWhitespace)
dtm <- DocumentTermMatrix(tweets6)
dtm
inspect(dtm[1:5, 1:10])
findFreqTerms(dtm,10)
findFreqTerms(dtm,5)
dtm_tfxidf <- weightTfIdf(dtm)
inspect(dtm_tfxidf[1:5, 100:103])
inspect(dtm_tfxidf[1:5, 100:103])
m <- as.matrix(dtm_tfxidf)
rownames(m) <- 1:nrow(m)
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5) m_norm <- norm_eucl(m)
t1 <- kmeans(m_norm, 10)
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5) m_norm <- norm_eucl(m)
norm_eucl <- function(m)
m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
m_norm <- norm_eucl(m)
m
t1 <- kmeans(m_norm, 10)
cl$cluster
t$cluster
t1
t1$cluster
t1$size
commments_out <- cbind(as.character(tweets$content), t1$cluster)
head(comments_out)
head(commments_out)
head(commments_out,2)
write.csv(commments_out, "Output_cluster.csv")
head(tweets$content)
dtm
head(tweets,2)
